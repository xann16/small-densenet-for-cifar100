{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maciej Manna\n",
    "### Project 1:\n",
    "# Model for classification of CIFAR100 dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approach:\n",
    "\n",
    "Classification using **DenseNet** model (see: G. Huang et al., [*Densely Connected Convolutional Networks*](https://arxiv.org/abs/1608.06993), 2018). Implementation in ```tensorflow 2``` inspired by [```densenet-tf2```](https://github.com/justincosentino/densenet-tf2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Import and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA IMPORT AND SPLIT PARAMETERS\n",
    "\n",
    "ds_name   = 'cifar100:3.*.*'\n",
    "ds_mean   = (129.3, 124.1, 112.4)\n",
    "ds_stddev = ( 68.2,  65.4,  70.4)\n",
    "ds_batch_size =    64\n",
    "ds_trn_size   = 45000\n",
    "ds_val_size   =  5000\n",
    "\n",
    "ds_split = [f\"train[0:{ds_trn_size}]\",\n",
    "            f\"train[{ds_trn_size}:{ds_trn_size + ds_val_size}]\",\n",
    "            f\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD CIFAR100 DATA USING 'tensorflow_datasets'\n",
    "\n",
    "(trn, val, tst), info = tfds.load(ds_name, as_supervised=True, split=ds_split, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESS DATASET\n",
    "\n",
    "def get_norm_fn(mean, stddev):\n",
    "    mean = tf.reshape(tf.constant(mean, dtype=tf.float64), [1, 1, 3])\n",
    "    stddev = tf.reshape(tf.constant(stddev, dtype=tf.float64), [1, 1, 3])\n",
    "\n",
    "    def normalize_image(img, label):\n",
    "        return tf.math.divide(tf.math.subtract(tf.cast(img, tf.float64), mean), stddev), label\n",
    "    return normalize_image\n",
    "\n",
    "trn = (trn.map(get_norm_fn(ds_mean, ds_stddev), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .cache().shuffle(1000).batch(ds_batch_size).repeat())\n",
    "\n",
    "val = (val.map(get_norm_fn(ds_mean, ds_stddev), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .batch(ds_batch_size).cache())\n",
    "\n",
    "tst = (tst.map(get_norm_fn(ds_mean, ds_stddev), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .batch(ds_batch_size).cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'CIFAR100' DATASET INFO:\n",
      "  - examples:                     60000 (training: 45000, validation: 5000, test: 10000)\n",
      "  - batch size used:              64\n",
      "  - number of separate classes:   100\n",
      "  - features:                     image, label\n",
      "        - image - data type: <dtype: 'uint8'>, shape: (32, 32, 3)\n",
      "        - label - data type: <dtype: 'int64'>\n"
     ]
    }
   ],
   "source": [
    "# BASIC DATASET PARAMETERS\n",
    "\n",
    "batch_size = ds_batch_size\n",
    "\n",
    "trn_size   = ds_trn_size\n",
    "val_size   = ds_val_size\n",
    "tst_size   = info.splits['test'].num_examples\n",
    "\n",
    "ds_size    = trn_size + val_size + tst_size\n",
    "\n",
    "ds_keys    = info.supervised_keys\n",
    "\n",
    "lbl_type   = info.features['label'].dtype\n",
    "img_type   = info.features['image'].dtype\n",
    "img_shape  = info.features['image'].shape\n",
    "\n",
    "n_classes  = info.features['label'].num_classes\n",
    "\n",
    "print(f\"'CIFAR100' DATASET INFO:\")\n",
    "print(f\"  - examples:                     {ds_size} (training: {trn_size}, validation: {val_size}, test: {tst_size})\")\n",
    "print(f\"  - batch size used:              {batch_size}\")\n",
    "print(f\"  - number of separate classes:   {n_classes}\")\n",
    "print(f\"  - features:                     {', '.join(ds_keys)}\")\n",
    "print(f\"        - image - data type: {img_type}, shape: {img_shape}\")\n",
    "print(f\"        - label - data type: {lbl_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS AND SETTINGS\n",
    "eps = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK DEFINITIONS\n",
    "\n",
    "# === DEFINITION OF TRANSITION BLOCKS === #\n",
    "\n",
    "def transition_block(x, reduction, name, dropout):\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3, epsilon=eps, name=name + '.batchnorm')(x)\n",
    "    x = tf.keras.layers.ReLU(name=name + '.relu')(x)\n",
    "    x = tf.keras.layers.Conv2D(filters=int(x.shape[-1] * reduction),\n",
    "                               kernel_size=(1, 1),\n",
    "                               strides=(1, 1),\n",
    "                               padding=\"valid\",\n",
    "                               use_bias=False,\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(l=1e-4),\n",
    "                               kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                               name=name + '.conv2d')(x)\n",
    "    \n",
    "    if dropout > 0:\n",
    "        x = tf.keras.layers.Dropout(rate=dropout, name=name + '.dropout')\n",
    "        \n",
    "    x = tf.keras.layers.AvgPool2D(pool_size=(2, 2),\n",
    "                                  strides=(2, 2),\n",
    "                                  padding=\"valid\",\n",
    "                                  name=name + '.avgpool')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "def final_transition_block(x, name):\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3, epsilon=eps, name=name + '.batchnorm')(x)\n",
    "    x = tf.keras.layers.ReLU(name=name + '.relu')(x)\n",
    "    x = tf.keras.layers.GlobalAvgPool2D(name=name + '.avgpool')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "# === DEFINITION OF CONVOLUTION BLOCK === #\n",
    "\n",
    "def convolution_block(x, growth_rate, name, dropout):\n",
    "\n",
    "    # BOTTLENECK\n",
    "    y = tf.keras.layers.BatchNormalization(axis=3, epsilon=eps, name=name + '.batchnorm_0')(x)\n",
    "    y = tf.keras.layers.ReLU(name=name + '.relu_0')(y)\n",
    "    y = tf.keras.layers.Conv2D(filters=4 * growth_rate,\n",
    "                               kernel_size=(1, 1),\n",
    "                               strides=(1, 1),\n",
    "                               padding=\"valid\",\n",
    "                               use_bias=False,\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(l=1e-4),\n",
    "                               kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                               name=name + '.conv2d_1')(y)\n",
    "    if dropout > 0:\n",
    "        x = tf.keras.layers.Dropout(rate=dropout, name=name + '.dropout_1')\n",
    "        \n",
    "    y = tf.keras.layers.BatchNormalization(axis=3, epsilon=eps, name=name + '.batchnorm_1')(y)\n",
    "    y = tf.keras.layers.ReLU(name=name + '.relu_1')(y)\n",
    "\n",
    "    \n",
    "    y = tf.keras.layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name=name + '.padding')(y)\n",
    "    y = tf.keras.layers.Conv2D(filters=growth_rate,\n",
    "                               kernel_size=(3, 3),\n",
    "                               strides=(1, 1),\n",
    "                               padding=\"valid\",\n",
    "                               use_bias=False,\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(l=1e-4),\n",
    "                               kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                               name=name + '.conv2d_2')(y)\n",
    "\n",
    "    if dropout > 0:\n",
    "        x = tf.keras.layers.Dropout(rate=dropout, name=name + '.dropout_2')\n",
    "    \n",
    "    x = tf.keras.layers.Concatenate(axis=3, name=name + 'cat')([x, y])\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "# === DEFINITION OF DENSE BLOCK === #\n",
    "\n",
    "def dense_block(x, growth_rate, num_per_block, name, dropout):\n",
    "    for i in range(num_per_block):\n",
    "        x = convolution_block(x, growth_rate, name=f\"{name}.block_{i + 1}\", dropout=dropout)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENSENET DEFINITION (for 32x32 images of CIFAR)\n",
    "\n",
    "def densenet(depth = 100, growth_rate = 12, reduction = 0.5, dropout = 0.0, summary=True):\n",
    "    num_classes = n_classes\n",
    "    num_per_block = ((depth - 4) // 3) // 2\n",
    "\n",
    "    # INPUT LAYER\n",
    "    x_in = tf.keras.Input(shape=(32, 32, 3), name=\"input\")\n",
    "\n",
    "    # INITIAL CONVOLUTION LAYER\n",
    "    x = tf.keras.layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name=\"init.padding\")(x_in)\n",
    "    x = tf.keras.layers.Conv2D(filters=growth_rate * 2,\n",
    "                               kernel_size=(3, 3),\n",
    "                               strides=(1, 1),\n",
    "                               padding=\"valid\",\n",
    "                               use_bias=False,\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(l=1e-4),\n",
    "                               kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                               name=\"init.conv2d\")(x)\n",
    "\n",
    "    # FIRST BLOCK\n",
    "    x = dense_block(x, growth_rate, num_per_block, \"dense_1\", dropout=dropout)\n",
    "    x = transition_block(x, reduction, \"trans_1\", dropout=dropout)\n",
    "    \n",
    "    # SECOND BLOCK\n",
    "    x = dense_block(x, growth_rate, num_per_block, \"dense_2\", dropout=dropout)\n",
    "    x = transition_block(x, reduction, \"trans_2\", dropout=dropout)\n",
    "\n",
    "    # THIRD (FINAL) BLOCK\n",
    "    x = dense_block(x, growth_rate, num_per_block, \"dense_3\", dropout=dropout)\n",
    "    x = final_transition_block(x, \"trans_3\")\n",
    "\n",
    "    # OUTPUT LAYER\n",
    "    x_out = tf.keras.layers.Dense(units=num_classes,\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(l=1e-4),\n",
    "                              name=\"output\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=x_in, outputs=x_out, name=\"densenet\")\n",
    "\n",
    "    if summary:\n",
    "        model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic survey of hyperparameters and number of model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETER SETUPS\n",
    "\n",
    "PARAMS = {\n",
    "    'default': {\n",
    "        'depth': 100,\n",
    "        'growth_rate': 12,\n",
    "        'reduction': 0.5,\n",
    "        'dropout': 0.0\n",
    "    },\n",
    "    'shallow_50': {\n",
    "        'depth': 50,\n",
    "        'growth_rate': 12,\n",
    "        'reduction': 0.5,\n",
    "        'dropout': 0.0\n",
    "    },\n",
    "    'shallow_40': {\n",
    "        'depth': 40,\n",
    "        'growth_rate': 12,\n",
    "        'reduction': 0.5,\n",
    "        'dropout': 0.0\n",
    "    },\n",
    "    'shallow_35': {\n",
    "        'depth': 35,\n",
    "        'growth_rate': 12,\n",
    "        'reduction': 0.5,\n",
    "        'dropout': 0.0\n",
    "    },\n",
    "    'grow_8': {\n",
    "        'depth': 100,\n",
    "        'growth_rate': 8,\n",
    "        'reduction': 0.5,\n",
    "        'dropout': 0.0\n",
    "    },\n",
    "    'grow_5': {\n",
    "        'depth': 100,\n",
    "        'growth_rate': 5,\n",
    "        'reduction': 0.5,\n",
    "        'dropout': 0.0\n",
    "    },\n",
    "    'grow_5_adj': {\n",
    "        'depth': 97,\n",
    "        'growth_rate': 5,\n",
    "        'reduction': 0.5,\n",
    "        'dropout': 0.0\n",
    "    },\n",
    "    'mixed_d64_g8': {\n",
    "        'depth': 64,\n",
    "        'growth_rate': 8,\n",
    "        'reduction': 0.5,\n",
    "        'dropout': 0.0\n",
    "    },\n",
    "    'mixed_d56_g8': {\n",
    "        'depth': 56,\n",
    "        'growth_rate': 8,\n",
    "        'reduction': 0.5,\n",
    "        'dropout': 0.0\n",
    "    },\n",
    "    'mixed_d128_g4': {\n",
    "        'depth': 128,\n",
    "        'growth_rate': 4,\n",
    "        'reduction': 0.5,\n",
    "        'dropout': 0.0\n",
    "    },\n",
    "    'mixed_d122_g4': {\n",
    "        'depth': 122,\n",
    "        'growth_rate': 4,\n",
    "        'reduction': 0.5,\n",
    "        'dropout': 0.0\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of parameters in different setups:\n",
    "\n",
    "Note: max number of parameters allowed - 150,000.\n",
    "\n",
    " - ```default``` - 824,020\n",
    "\n",
    " - ```shallow_50``` - 238,714\n",
    " - ```shallow_40``` - 193,180\n",
    " - ```shallow_35``` - 151,546 <<<\n",
    " \n",
    " - ```grow_8``` - 384,692\n",
    " - ```grow_5``` - 162,966\n",
    " - ```grow_5_adj``` - 146,866 <<<\n",
    " \n",
    " - ```mixed_d64_g8``` - 187,124\n",
    " - ```mixed_d56_g8``` - 135,604 <<<\n",
    " - ```mixed_d128_g4``` - 157,092\n",
    " - ```mixed_d122_g4``` - 144,594 <<<\n",
    " \n",
    " Choice of four setups that meet parameter criteria, with following depth--growth rate tradeoffs:\n",
    "  - #1 - depth: 122, gr:  4\n",
    "  - #2 - depth:  97, gr:  5\n",
    "  - #3 - depth:  56, gr:  8\n",
    "  - #4 - depth:  35, gr: 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = densenet(**PARAMS['mixed_d120_g4'])\n",
    "\n",
    "choices = [('d122_gr4','mixed_d122_g4'),\n",
    "           ('d97_gr5', 'grow_5_adj'),\n",
    "           ('d56_gr8' ,'mixed_d56_g8'),\n",
    "           ('d35_gr12', 'shallow_35')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING SETTINGS\n",
    "\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "\n",
    "#base_dir='~/.dlearn/'\n",
    "#base_dir='.dlearn'\n",
    "\n",
    "base_dir = '.'\n",
    "\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DATA DIRS\n",
    "\n",
    "base_dir = os.path.expanduser(base_dir)\n",
    "assert os.path.isdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING FOR MODEL (per key)\n",
    "\n",
    "def train_model(key, base_dir, epochs=2):\n",
    "    \n",
    "    # DIRS SETUP\n",
    "    base_dir = os.path.join(base_dir, key)\n",
    "    if not os.path.exists(base_dir):\n",
    "        os.mkdir(base_dir)\n",
    "    \n",
    "    chk_dir = os.path.join(base_dir, 'checkpoints')\n",
    "    if not os.path.exists(chk_dir):\n",
    "        os.mkdir(chk_dir)\n",
    "\n",
    "    log_dir = os.path.join(base_dir, 'logs')\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.mkdir(log_dir)\n",
    "    \n",
    "    strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    #strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "    with strategy.scope():        \n",
    "        par_key = [par_key for ckey, par_key in choices if ckey == key][0]\n",
    "        model = densenet(summary=False, **PARAMS[par_key])\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum, nesterov=True)\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    \n",
    "    def scheduler(epoch):\n",
    "        if epoch < epochs * 0.5:\n",
    "            return learning_rate\n",
    "        if epoch >= epochs * 0.5 and epoch < epochs * 0.75:\n",
    "            return FLAGS.lr / 10.0\n",
    "        return FLAGS.lr / 100.0\n",
    "    \n",
    "    # CALLBACKS\n",
    "    lr_sched_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, profile_batch=0)\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        #filepath=os.path.join( chk_dir, \"checkpoint-weights-{epoch:02d}-{val_loss:.2f}.ckpt\"),\n",
    "        filepath=os.path.join( chk_dir, \"checkpoint-weights-{epoch:02d}.ckpt\"),\n",
    "        save_weights_only=True, verbose=1)\n",
    "        #save_weights_only=True, verbose=1, save_best_only=True)\n",
    "    \n",
    "    # ACTUAL TRAINING\n",
    "    _ = model.fit(x=trn,\n",
    "                  steps_per_epoch=trn_size // batch_size,\n",
    "                  epochs=epochs,\n",
    "                  callbacks=[lr_sched_callback, tensorboard_callback, checkpoint_callback],\n",
    "                  validation_data=val,\n",
    "                  verbose=2)\n",
    "        \n",
    "    # MODEL EVALUATION\n",
    "    model.load_weights(tf.train.latest_checkpoint(chk_dir))\n",
    "    results = model.evaluate(tst)\n",
    "    print(\"\\ntest loss {}, test acc: {}\".format(results[0], results[1]))\n",
    "    \n",
    "    return model, chk_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating number of allowed training epochs\n",
    "\n",
    "Each model is trained for two epochs (first one is always slightly longer), and time of training second one is retrieved. On that base it is estimated how much epochs can be used for training to fit into 30 minute limit (minus one, to accomodate for longer first epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTIMNATING No OF EPOCHS FOR 'd122_gr4' SETUP\n",
    "_ = train_model('d122_gr4', base_dir)\n",
    "tt1 = 95\n",
    "epochs1 = int((30 * 60) / tt1) # 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTIMNATING No OF EPOCHS FOR 'd97_gr5' SETUP\n",
    "_ = train_model('d97_gr5', base_dir)\n",
    "tt2 = 79\n",
    "epochs2 = int((30 * 60) / tt2) # 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTIMNATING No OF EPOCHS 'd56_gr8' SETUP\n",
    "_ = train_model('d56_gr8', base_dir)\n",
    "tt3 = 50\n",
    "epochs3 = int((30 * 60) / tt3) # 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESTIMNATING No OF EPOCHS FOR 'd35_gr12' SETUP\n",
    "_ = train_model('d35_gr12', base_dir)\n",
    "tt4 = 42\n",
    "epochs4 = int((30 * 60) / tt4) # 41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proper Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND EVALUATION OF MODEL FOR 'd122_gr4' SETUP\n",
    "m1, cd1 = train_model('d122_gr4', base_dir, epochs=epochs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND EVALUATION OF MODEL FOR 'd97_gr5' SETUP\n",
    "m2, cd2 = train_model('d97_gr5', base_dir, epochs=epochs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND EVALUATION OF MODEL FOR 'd56_gr8' SETUP\n",
    "m3, cd3 = train_model('d56_gr8', base_dir, epochs=epochs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND EVALUATION OF MODEL FOR 'd35_gr12' SETUP\n",
    "m4, cd4 = train_model('d35_gr12', base_dir, epochs=epochs4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Summary\n",
    "\n",
    "|       -        | Training Set |     -    | Validation Set |     -    | Test Set |    -     |\n",
    "|---------------:|:------------:|:--------:|:--------------:|:--------:|:--------:|:--------:|\n",
    "|     Model      |     Loss     |    Acc   |      Loss      |    Acc   |   Loss   |    Acc   |\n",
    "| ```d122_gr4``` |     1.91     |   0.60   |      2.56      |   0.47   |  2.5226  |  0.4746  |\n",
    "|  ```d97_gr5``` |     1.88     |   0.61   |      2.54      |   0.48   |  2.5653  |  0.4789  |\n",
    "|  ```d56_gr8``` |     1.81     |   0.63   |      2.74      |   0.46   |  2.7928  |  0.4583  |\n",
    "| ```d35_gr12``` |     1.81     |   0.65   |      2.59      |   0.50   |  2.5738  |  0.5048  |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
